{"cells":[{"cell_type":"markdown","source":["# TensorFlow"],"metadata":{}},{"cell_type":"markdown","source":["In this tutorial we will go through the basics of TensorFlow, from installation to creating, running, and saving simple computational graphs."],"metadata":{}},{"cell_type":"markdown","source":["### Table of Contents <br \\>\n- 1  What is TensorFlow?\n- 2 How does TensorFlow work?\n- 3 Creating Your First Graph and Running It in a Session \n- 4 Managing Graphs \n- 5 Lifecycle of a Node Value \n- 6 Linear Regression with TensorFlow   \n- 7 Implementing Gradient Descent  \n- 8 Feeding Data to the Training Algorithm  \n- 9 Saving and Restoring Models \n- 10 TensorFlow in Use"],"metadata":{}},{"cell_type":"markdown","source":["### 1. What is TensorFlow?"],"metadata":{}},{"cell_type":"markdown","source":["TensorFlow is an open source complex library for distributed numerical computation using data flow graphs. It makes it possible to train and run very large neural networks efficiently by distributing the computations across potentially thousands of multi-GPU servers. \n\nTensorFlow was created at Google and supports many of their large-scale Machine Learning applications. TensorFlow can train a network with millions of parameters on a training set composed of billions of instances with millions of features each. It was open-sourced in November 2015.\n\nFor more info check the information on https://www.tensorflow.org/\n\nWhat are tensors? \n\nTensorFlow, as the name indicates, is a framework to define and run computations involving tensors. A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes.\n\nWhen writing a TensorFlow program, the main object you manipulate and pass around is the tf.Tensor. A tf.Tensor object represents a partially defined computation that will eventually produce a value. TensorFlow programs work by first building a graph of tf.Tensor objects, detailing how each tensor is computed based on the other available tensors and then by running parts of this graph to achieve the desired results.\n\nA tf.Tensor has the following properties:\n\n  - a data type (float32, int32, or string, for example)\n  - a shape\n\nEach element in the Tensor has the same data type, and the data type is always known. \n\nMore info on tensors on this link https://www.tensorflow.org/programmers_guide/tensors"],"metadata":{}},{"cell_type":"markdown","source":["### 2. How does TensorFlow work?"],"metadata":{}},{"cell_type":"markdown","source":["`TensorFlow` library is used for numerical computation and fine-tuned for large-scale Machine Learning. We first define a graph of computations to perform and then `TensorFlow` takes that graph and runs it using optimized C++ code.  <br \\> <br \\> A `TensorFlow` program consists of two parts: <br \\> 1- The construction phase builds a computation graph representing the ML model and the computations required to train it. <br \\> 2- The execution phase runs a loop that evaluates a training step repeatedly, gradually improving the model parameters.\n\nFor tf graphs check more on  https://www.tensorflow.org/programmers_guide/graphs \n\nFor tf operations check more on https://www.tensorflow.org/api_docs/python/tf/Operation"],"metadata":{}},{"cell_type":"markdown","source":["##### Install Packages"],"metadata":{}},{"cell_type":"code","source":["%sh /databricks/python3/bin/pip3 install tensorflow"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### 3. Creating Your First Graph and Running It in a Session"],"metadata":{}},{"cell_type":"markdown","source":["Create a computation graph."],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\n\nx = tf.Variable(3, name = \"x\")\ny = tf.Variable(4, name = \"y\")\nf = x*x*y + y + 2"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Important! - The most important thing to understand is that this code does not actually perform any computation, even though it looks like it does (especially the\nlast line). It just creates a computation graph. In fact, even the variables are not initialized yet. To evaluate this graph, you need to open a TensorFlow session and use it to initialize the variables and evaluate f. A TensorFlow session takes care of placing the operations onto devices such as CPUs and GPUs and running them, and it holds all the variable values."],"metadata":{}},{"cell_type":"markdown","source":["Create a session, initialize the variables and evaluate. Then, close the session with `f` to free up resources."],"metadata":{}},{"cell_type":"code","source":[" sess = tf.Session()\n sess.run(x.initializer)\n sess.run(y.initializer)\n result = sess.run(f)\n print(result)\n sess.close()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Set the session as the default session inside the `with` block.  `x.initializer.run()` is equivalent to  `tf.get_default_session().run(x.initializer)`, and similarly `f.eval()` is equivalent to`tf.get_default_session().run(f)`. The session is automatically closed at the end of the block."],"metadata":{}},{"cell_type":"code","source":["with tf.Session() as sess:\n    x.initializer.run()\n    y.initializer.run()\n    result = f.eval()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["This code performs the same set of operations as above but in an optimized way as it does not repeat `sess.run()` and does the initialization automatically. \n\nNotice that the initialization is not done immediately. It first creates a node in the graph  that will initialize all the variables when it is run."],"metadata":{}},{"cell_type":"code","source":["init = tf.global_variables_initializer()  # prepare an init node\n\nwith tf.Session() as sess:\n    init.run()  # actually initialize all the variables\n    result = f.eval()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Create an `InteractiveSession`. Once it is created it automatically sets itself as the default session. There is no need for a `with` block. \n\nNote: The only difference from a regular Session is that when an `InteractiveSession` is created it automatically sets itself as the default session."],"metadata":{}},{"cell_type":"code","source":["sess = tf.InteractiveSession()\ninit.run()\nresult = f.eval()\nprint(result)\nsess.close()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["### 4. Managing Graphs"],"metadata":{}},{"cell_type":"markdown","source":["A node is a {matrix, tensor, vector, scalar} value. Any node created is automatically added to the default graph.\n\n__What if we do not want this to happen?__\n \n We can define temporal default graphs inside `with` blocks so that the node created is added in the new graph."],"metadata":{}},{"cell_type":"markdown","source":["Create a node."],"metadata":{}},{"cell_type":"code","source":["x1 = tf.Variable(1)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Check if the node is added to the default graph."],"metadata":{}},{"cell_type":"code","source":["x1.graph is tf.get_default_graph()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Create a new graph and temporarily make it the default graph inside a `with` block."],"metadata":{}},{"cell_type":"code","source":["graph = tf.Graph()\nwith graph.as_default():\n    x2 = tf.Variable(2)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["More on `tf.Variable()` can be found on https://www.tensorflow.org/api_docs/python/tf/Variable"],"metadata":{}},{"cell_type":"markdown","source":["Check if the new node is added to the new graph."],"metadata":{}},{"cell_type":"code","source":["x2.graph is graph"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["Check if the new node is added to the old graph."],"metadata":{}},{"cell_type":"code","source":["x2.graph is tf.get_default_graph()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["Reset the default graph."],"metadata":{}},{"cell_type":"code","source":["tf.reset_default_graph()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["### 5. Lifecycle of a Node Value"],"metadata":{}},{"cell_type":"markdown","source":["When we evaluate a node, `TensorFlow` automatically determines the set of nodes that depends on and it evaluates these nodes first."],"metadata":{}},{"cell_type":"markdown","source":["Define a simple graph."],"metadata":{}},{"cell_type":"code","source":["w = tf.constant(3)\nx = w + 2\ny = x + 5\nz = x * 3"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["Start the session and run the graph to evaluate `y` and `z`."],"metadata":{}},{"cell_type":"code","source":["with tf.Session() as sess:\n    print(y.eval())  # 10\n    print(z.eval())  # 15"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["This code performs the same set of operations as above but is more efficient because it evaluates both `y` and `z` in one run. Unlike the previous code here, `w` and `x` are evaluated only once."],"metadata":{}},{"cell_type":"code","source":["with tf.Session() as sess:\n    y_val, z_val = sess.run([y, z])\n    print(y_val)  # 10\n    print(z_val)  # 15"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["### 6. Linear Regression with `TensorFlow`"],"metadata":{}},{"cell_type":"markdown","source":["`TensorFlow` operations (called ops) can take any number of inputs and produce any number of outputs. \n- For example, multiplication ops takes two inputs and produces one output. \n- Source ops like constants and variables take no inputs. \n\nThe inputs and outputs are multidimensional arrays, called tensors. Tensors have a type and a shape."],"metadata":{}},{"cell_type":"markdown","source":["Fetch the California housing dataset and add an extra bias input feature (x0 = 1) to all training instances using `NumPy`. In the linear regression model, slope will become the weight and the constant will act as bias. The weights and biases are the parameters you are going to optimize in order to get a good and accurate model."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nfrom sklearn.datasets import fetch_california_housing\n\nhousing = fetch_california_housing()\nm, n = housing.data.shape\nhousing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["Note : np.ones((2, 1))\n\n  array ([[ 1.],\n\n         [ 1.]])"],"metadata":{}},{"cell_type":"markdown","source":["Print the shape of `housing` data."],"metadata":{}},{"cell_type":"code","source":["print(m, n)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["Creates two `TensorFlow` constant nodes, X and y."],"metadata":{}},{"cell_type":"code","source":["X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\ny = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["These matrix functions — `transpose()`, `matmul()`, and `matrix_inverse()` - do not perform any computations immediately; instead, they create nodes in the graph that will perform them when the graph is run. The definition of `theta` corresponds to the Normal Equation ((XT · X)–1 · XT · y). By convention, the Greek letter θ (theta) is frequently used to represent model parameters.\n\nNote: \n- XT is the transpose of X  \n- check http://mathworld.wolfram.com/MatrixInverse.html for matrix_inverse\n- matmul() is matrix multiplication like the dot product in numpy\n- reshape(-1,1) the unspecified value is -1"],"metadata":{}},{"cell_type":"code","source":["XT = tf.transpose(X)\ntheta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["d Create a session and use it to evaluate `theta`."],"metadata":{}},{"cell_type":"code","source":["with tf.Session() as sess:\n    theta_value = theta.eval()"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["Print the `theta_value`."],"metadata":{}},{"cell_type":"code","source":["theta_value"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["### 7. Implementing Gradient Descent"],"metadata":{}},{"cell_type":"markdown","source":["Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. There are optimizers that can speed up training large models tremendously compared to plain Gradient Descent.\n\nMore info on Gradient Descent: https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer"],"metadata":{}},{"cell_type":"markdown","source":["In this section we will use the Batch Gradient Descent instead of the Normal Equation. First we will manually compute the gradients, then we will use `TensorFlow`’s autodiff feature to let TensorFlow compute the gradients automatically. At the end, we will use a couple of TensorFlow’s out-of-the-box optimizers."],"metadata":{}},{"cell_type":"markdown","source":["##### Manually Computing the Gradients"],"metadata":{}},{"cell_type":"markdown","source":["When using Gradient Descent, remember that it is important to first normalize the input feature vectors, or else training may be much slower. You can do this using TensorFlow, NumPy, Scikit- Learn’s StandardScaler, or any other solution you prefer."],"metadata":{}},{"cell_type":"markdown","source":["Scale the feature vectors in order to use Gradient Descent."],"metadata":{}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled_housing_data = scaler.fit_transform(housing.data)\nscaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["Create a node in the graph that will generate a tensor containing random values, given its shape and value range using the `random_uniform()` function. \n\n- The `random_uniform()` function creates a node in the graph that will generate a tensor containing random values, given its shape and value range, much like NumPy’s rand() function.\n- The `assign()` function creates a node that will assign a new value to a variable. In this case, it implements the Batch Gradient Descent step θ(next step) = θ – η∇θMSE(θ)."],"metadata":{}},{"cell_type":"code","source":["n_epochs = 1000\nlearning_rate = 0.01\n\nX = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\ny = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\ntheta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\ny_pred = tf.matmul(X, theta, name=\"predictions\")\nerror = y_pred - y\nmse = tf.reduce_mean(tf.square(error), name=\"mse\")\ngradients = 2/m * tf.matmul(tf.transpose(X), error)\ntraining_op = tf.assign(theta, theta - learning_rate * gradients)"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["This loop executes the training step over and over again (`n_epochs` times), and every 100 iterations it prints out the current Mean Squared Error (mse). You should see the MSE go down at every iteration."],"metadata":{}},{"cell_type":"code","source":["init = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n\n    for epoch in range(n_epochs):\n        if epoch % 100 == 0:\n            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n        sess.run(training_op)\n\n    best_theta = theta.eval()"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["Print `best_theta`."],"metadata":{}},{"cell_type":"code","source":["best_theta"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":["##### Using Autodiff"],"metadata":{}},{"cell_type":"markdown","source":["This code is the same as the two cells above but replaces `gradients` = ... with the `gradients()` function to automatically compute the gradients. <br \\>\nThe gradients() function takes the `mse` and the `theta`, and creates a list of ops (one per variable) to compute the gradients of the op with regards to each variable. The gradients node will compute the gradient vector of the MSE with regards to theta."],"metadata":{}},{"cell_type":"code","source":["n_epochs = 1000\nlearning_rate = 0.01\n\nX = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\ny = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\ntheta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\ny_pred = tf.matmul(X, theta, name=\"predictions\")\nerror = y_pred - y\nmse = tf.reduce_mean(tf.square(error), name=\"mse\")\n\ngradients = tf.gradients(mse, [theta])[0] #notice the gradient from the previous code is changed \n\ntraining_op = tf.assign(theta, theta - learning_rate * gradients)\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init) \n\n    for epoch in range(n_epochs):\n        if epoch % 100 == 0:\n            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n        sess.run(training_op)\n\n    best_theta = theta.eval()"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":["Print `best_theta`."],"metadata":{}},{"cell_type":"code","source":["best_theta"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":["##### Using an Optimizer"],"metadata":{}},{"cell_type":"markdown","source":["The cell code above is used again replacing the `training_op`=... with code to use an optimizer as shown below."],"metadata":{}},{"cell_type":"code","source":["n_epochs = 1000\nlearning_rate = 0.01\n\nX = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\ny = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\ntheta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\ny_pred = tf.matmul(X, theta, name=\"predictions\")\nerror = y_pred - y\nmse = tf.reduce_mean(tf.square(error), name=\"mse\")\ngradients = tf.gradients(mse, [theta])[0] \n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate) #notice codeline added\ntraining_op = optimizer.minimize(mse) #notice the training_op from the previous code is changed \n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n\n    for epoch in range(n_epochs):\n        if epoch % 100 == 0:\n            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n        sess.run(training_op)\n\n    best_theta = theta.eval()"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"markdown","source":["Print `best_theta`."],"metadata":{}},{"cell_type":"code","source":["best_theta"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"markdown","source":["To use a different type of optimizer change the code line that defines the optimizer. This code is an example of how to define a momentum optimizer."],"metadata":{}},{"cell_type":"code","source":["optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n                                       momentum=0.9)"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"markdown","source":["### 8. Feeding Data to the Training Algorithm"],"metadata":{}},{"cell_type":"markdown","source":["In this section we will modify the previous code to implement Mini-batch Gradient Descent. In order to do this, we need to replace X and y at every iteration with the next mini-batch. The simplest way to do this is to use placeholder nodes. These nodes just output the data we tell them to output at runtime."],"metadata":{}},{"cell_type":"markdown","source":["Create a placeholder node `A` and node `B` = `A + 5`. When we evaluate B, we pass a `feed_dict` to the `eval()` method that specifies the value of `A`. `A` must be two-dimensional and there must be three columns. The number of rows can be any."],"metadata":{}},{"cell_type":"code","source":["A = tf.placeholder(tf.float32, shape=(None, 3))\nB = A + 5\nwith tf.Session() as sess:\n     B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n     B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"markdown","source":["Print `B_val_1`. Notice that `A` passed to `eval()` is two dimensional, three columns and one row."],"metadata":{}},{"cell_type":"code","source":["print(B_val_1)"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"markdown","source":["Print `B_val_2`. Notice that `A` passed to `eval()` is two dimensional, three columns and two rows."],"metadata":{}},{"cell_type":"code","source":["print(B_val_2)"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"markdown","source":["In order to implement Mini-batch Gradient Descent we change the definition of X and y in the construction phase to make them placeholder nodes."],"metadata":{}},{"cell_type":"code","source":["X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\ny = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"markdown","source":["Define the batch size and compute the total number of batches."],"metadata":{}},{"cell_type":"code","source":["batch_size = 100\nn_batches = int(np.ceil(m / batch_size))\nn_batches"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"markdown","source":["####Note: \nnp.ceil(-1.5, 0.2) is -1 and 0"],"metadata":{}},{"cell_type":"markdown","source":["Fetch the mini-batches one by one."],"metadata":{}},{"cell_type":"code","source":["def fetch_batch(epoch, batch_index, batch_size):\n    np.random.seed(epoch * n_batches + batch_index)  \n    indices = np.random.randint(m, size=batch_size)  # generate indices the highest is m the size is batch size for example; 1, 7, 8, 56, 23, 14, 10 if batch size 7 and m=60\n    X_batch = scaled_housing_data_plus_bias[indices] \n    y_batch = housing.target.reshape(-1, 1)[indices] \n    return X_batch, y_batch\n\nwith tf.Session() as sess:\n    sess.run(init)\n\n    for epoch in range(n_epochs):\n        for batch_index in range(n_batches):\n            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n\n    best_theta = theta.eval()"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"markdown","source":["### 9. Saving and Restoring Models"],"metadata":{}},{"cell_type":"markdown","source":["To save models with `TensorFlow`: <br \\ >\n1- Create a `saver` node at the end of the construction phase after all variable nodes are created. <br \\ >\n2- Call `save()` method in the execution phase passing it the session and path of the checkpoint file. (Use it whenever you change the model )"],"metadata":{}},{"cell_type":"code","source":["from tensorflow.python.framework import ops                      # library imported to reset the graph\nops.reset_default_graph()                                        # notice that we start with an empty graph\n\nn_epochs = 1000                                                                      \nlearning_rate = 0.01                                                            \n\nX = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")            \ny = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")            \ntheta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\ny_pred = tf.matmul(X, theta, name=\"predictions\")                                      \nerror = y_pred - y                                                                    \nmse = tf.reduce_mean(tf.square(error), name=\"mse\")                                    \noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)            \ntraining_op = optimizer.minimize(mse)                                                \n\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()                                        # create the saver node after all the variables nodes are created \n\nwith tf.Session() as sess:\n    sess.run(init)\n\n    for epoch in range(n_epochs):\n        if epoch % 100 == 0:                                    # checkpoint every 100 epochs\n            print(\"Epoch\", epoch, \"MSE =\", mse.eval())                                \n            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")  # use the save_path() to save the model\n        sess.run(training_op)\n    \n    best_theta = theta.eval()\n    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")    # use the save_path() to save the model "],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"markdown","source":["Print the `best_theta`."],"metadata":{}},{"cell_type":"code","source":["best_theta"],"metadata":{},"outputs":[],"execution_count":104},{"cell_type":"markdown","source":["__To restore models with `TensorFlow`: <br \\ >__\n\n1. Create a `saver` node at the end of the constructtion phase. <br \\ >\n2. Instead of initializing the variables using the `init` node, we call the `restore()` method of the `saver` object. <br \\> For example, use `saver.restore(sess, \"/tmp/my_model_final.ckpt\")` instead of `init` code."],"metadata":{}},{"cell_type":"markdown","source":["By default a `Saver` saves and restores all variables under their own name, but we can also specify which variables to save or restore, and under which names to save. For example, `saver = tf.train.Saver({\"w\": theta})` will save or restore only the `theta` variable under the name `'w'`."],"metadata":{}},{"cell_type":"markdown","source":["The `save()` method saves the structure of the graph in a second file with the same name plus a `.meta` extension. Use `tf.train.import_meta_graph()` to load this graph structure. This adds the graph to the default graph, and returns a `saver` instance. The instance can be used to restore the graph’s state."],"metadata":{}},{"cell_type":"code","source":["ops.reset_default_graph() # notice that we start with an empty graph\n\nsaver = tf.train.import_meta_graph(\"/tmp/my_model_final.ckpt.meta\")  # this loads the graph structure\ntheta = tf.get_default_graph().get_tensor_by_name(\"theta:0\") \n\nwith tf.Session() as sess:\n    saver.restore(sess, \"/tmp/my_model_final.ckpt\")  # this restores the graph's state\n    best_theta_restored = theta.eval() "],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"markdown","source":["Print `best_theta_restored`."],"metadata":{}},{"cell_type":"code","source":["best_theta_restored"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"markdown","source":["`np.allclose()` returns true if the two arrays are equal element-wise within a tolerance. This means that we can load a previously saved model without having to have the code that built it."],"metadata":{}},{"cell_type":"code","source":["np.allclose(best_theta, best_theta_restored) "],"metadata":{},"outputs":[],"execution_count":112},{"cell_type":"markdown","source":["### 10. Tensorflow in Use"],"metadata":{}},{"cell_type":"markdown","source":["Some of the current uses of Tensorflow are:\n\n- ### 1. Deep Speech\n\n  -Organization: Mozilla\n  \n  -Domain: Speech Recognition\n  \n  -Description: A TensorFlow implementation motivated by Baidu's Deep Speech architecture.\n\n- ### 2. RankBrain\n\n  -Organization: Google\n  \n  -Domain: Information Retrieval\n  \n  -Description: A large-scale deployment of deep neural nets for search ranking on www.google.com\n  \n  -More info: \"Google Turning Over Its Lucrative Search to AI Machines\"\n\n- ### 3. Inception Image Classification Model\n\n  -Organization: Google\n  \n  -Description: Baseline model and follow on research into highly accurate computer vision models, starting with the model that won the 2014 Imagenet image classification challenge\n\n\n- ### -  4. SmartReply\n\n  -Organization: Google\n  \n  -Description: Deep LSTM model to automatically generate email responses\n\n- ### 5. Massively Multitask Networks for Drug Discovery\n\n  -Organization: Google and Stanford University\n  \n  -Domain: Drug discovery\n  \n  -Description: A deep neural network model for identifying promising drug candidates\n  \n\n_Note: For more details check https://www.tensorflow.org/about/uses_"],"metadata":{}}],"metadata":{"name":"Introduction","notebookId":210962},"nbformat":4,"nbformat_minor":0}