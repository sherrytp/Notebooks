{"cells":[{"cell_type":"markdown","source":["# Introduction to Tweepy"],"metadata":{}},{"cell_type":"code","source":["import sys\nsys.version"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%sh /databricks/python3/bin/pip3 install tweepy"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## 1. Twitter API and retrieving tokens"],"metadata":{}},{"cell_type":"markdown","source":["`Tweepy` is a library for accessing the Twitter API. This is how Twitter API works."],"metadata":{}},{"cell_type":"markdown","source":["![alt](https://cms-assets.tutsplus.com/uploads/users/317/posts/22192/image/streaming-intro-1_1.png)"],"metadata":{}},{"cell_type":"markdown","source":["In order to get data from Twitter, we need to have consumer and access tokens. The consumer key and consumer secret are values you receive from the server when you register your application and these are used to verify that you're a Twitter user. The consumer key is how Twitter identifies your application when you send requests. You will need read and write access to post a tweet.\n\nAccess tokens are to be used to make API request on your own account's behalf to Twitter and access tokens secret is used to encrypt data from the server.\n\n(More on OAuth: https://techrangers.cdl.ucf.edu/oauth-python-tutorial.php)"],"metadata":{}},{"cell_type":"markdown","source":["To get consumer token and access token, proceed to following instructions."],"metadata":{}},{"cell_type":"markdown","source":["- Create a Twitter account or use your existing one.\n- Go to https://apps.twitter.com/ and log in with your account.\n- Click on Create your app and submit your phone number. A valid phone number is required for the verification process. You can use your mobile phone number for one account only.\n- Fill the form, agree to the terms and conditions, and create your Twitter application.\n- Go to the Keys and Access Tokens tab, save your API key, and API secret and then click on - - \n- Create my access token to obtain the Access token and Access token secret. These four elements will be required to establish a connection with the API."],"metadata":{}},{"cell_type":"code","source":["url_rest = \"https://api.twitter.com/1.1/search/tweets.json\" \nurl_streaming = \"https://stream.twitter.com/1.1/statuses/sample.json\" "],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## 2. Tweepy library\n- http://www.tweepy.org"],"metadata":{}},{"cell_type":"markdown","source":["## 2.1. OAuth\n\nOAuth is Open Authentication Protocol which lets you act on a person's behalf and do things to a website."],"metadata":{}},{"cell_type":"code","source":["consumer_key        = u'rfbbqTwbEdU7FfhEM5qhWdQSn'\nconsumer_secret     = u'AXbEXOoIOKIRPPOGXizYCY8zrPpg73aznPwqlbiPBtqyNjX1ZZ'\naccess_token        = u'50464413-petgMmE6qEKzW33wN7AdMMZODxME9KJSSADyKx99r' \naccess_token_secret = u'JBVgy7RcTnHswLyIaOgkLNMRiIsrICBt7fB4CxFRd5Wra' "],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["`OAuthHandler()` is a function that authorize our app to access Twitter on our behalf. `set_access_token()` initializes access token and token secret. Then your authorization is passed to `.API` function to access Twitter data."],"metadata":{}},{"cell_type":"code","source":["import tweepy\n\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\n\napi = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n\n#Error handling\ntry:\n  redirect_url = auth.get_authorization_url()\nexcept tweepy.TweepError:\n  print ('Error! Failed to get request token.')"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["## 2.2. API Examples"],"metadata":{}},{"cell_type":"markdown","source":["### 1. Timeline methods\n\nTimeline methods help us access timeline information of authenticated user or the user specified."],"metadata":{}},{"cell_type":"markdown","source":["`.home_timeline()` pulls the most recents statuses, including retweets, on your Twitter timeline and default number of tweets that returns is 20 which can be modified."],"metadata":{}},{"cell_type":"code","source":["number_tweets=30\npagination=1\npublic_tweets = api.home_timeline(count=number_tweets, page=pagination)\nprint(\"number of tweets:\", len(public_tweets))\nprint(\"class of the object:\", type(public_tweets))\npublic_tweets[0]._json"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Each one of the JSON attributes is turned into an individual class member in Tweepyâ€™s tweet object. The following is the function that lists all the class members."],"metadata":{}},{"cell_type":"code","source":["def PrintMembers(obj):\n    for attribute in dir(obj):\n        \n        #We don't want to show built in methods of the class\n        if not attribute.startswith('__'):\n            print(attribute)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["PrintMembers(public_tweets[0])"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["We can access each attribute by mounting the class name to the object."],"metadata":{}},{"cell_type":"code","source":["print(\"tweet:\", public_tweets[0].text)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["`.user_timeline()` accesses the most recent statuses, including retweets, posted from the specified user. The default value of number of tweets to retrieve can be modified and pagination can be used."],"metadata":{}},{"cell_type":"code","source":["username=\"hugobowne\"\nnumber_tweets=30\nuser_tweets=api.user_timeline(username, count=number_tweets)\n\nfor tweet in user_tweets:\n  print(tweet.text)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["### 2. User methods\n\nUser methods are designed to work with user and followers information."],"metadata":{}},{"cell_type":"markdown","source":["####Extracting followers list\n\n`.followers()` returns information of people who follows specified user and the results are ordered in the most recent followers first in\nwhich they're added 20 at a time. To mount more followers list, we need to use `cursor` to navigate to the next requests."],"metadata":{}},{"cell_type":"code","source":["followers=api.followers(username)\nlen(followers)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Following function can be used to pass the next cursor to the API and extract followers list automatically. After certain number of follower extraction, the extraction reaches its limit and takes time to sleep."],"metadata":{}},{"cell_type":"code","source":["import json\nimport time\n\nfollowers=[]\ndef process_or_store_followers(follower):\n    followers.append(follower)\n    \ndef limit_handled(cursor):\n  while True:\n    try:\n      yield cursor.next()\n    except tweepy.RateLimitError:\n      time.sleep(15 * 60)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["followers_count=0\nuser_name=\"hugobowne\"\nfor follower in limit_handled(tweepy.Cursor(api.followers, \n                              screen_name=user_name, \n                              rpp=100,\n                             ).items()):  \n  process_or_store_followers(follower._json)\n  followers_count+=1\n  print(\"Downloaded {0} followers\".format(followers_count))"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["Display `followers` list."],"metadata":{}},{"cell_type":"code","source":["followers"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["print(len(followers))\nfollowers[1]"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["Saving extracted followers list as `pickle` file."],"metadata":{}},{"cell_type":"code","source":["import pickle\n\npickle.dump(followers, open( \"/dbfs/FileStore/tmp/followers.pkl\", \"wb\" ) )"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["%sh ls -lh /dbfs/FileStore/tmp/*.pkl"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["Loading saved pickle file as `hugo_followers`"],"metadata":{}},{"cell_type":"code","source":["import pickle\n\nhugo_followers = pickle.load(open( \"/dbfs/FileStore/tmp/followers.pkl\", \"rb\" ) )"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["len(hugo_followers), type(hugo_followers), type(hugo_followers[0])"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["hugo_followers[0]"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["hugo_followers[0].keys()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["We can use `pandas` library to create dataframe from list of dictionaries by assigning column names to `columns` parameter."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\n\ncol_names=['name', 'screen_name', 'followers_count', 'friends_count', 'favourites_count', \n           'status_count','created_at','language','location']\nhugo_follower_data = pd.DataFrame.from_records(hugo_followers, columns=col_names)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["hugo_follower_data.head()"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["Saving `hugo_follower_data` dataframe as pickle file."],"metadata":{}},{"cell_type":"code","source":["import pickle\n\npickle.dump(hugo_follower_data, open( \"/dbfs/FileStore/tmp/hugo_follower_data.pkl\", \"wb\" ) )"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["####Extracting friends list\n\n`.friends()` returns information of users that specified user is following. It returns the most recent following first in which they're added 20 at a time. To mount more friends list, `cursor` is used to navigate to the next requests."],"metadata":{}},{"cell_type":"code","source":["import json\nimport time\n\nfollowing=[]\ndef process_or_store_following(friend):\n    following.append(friend)\n    \ndef limit_handled(cursor):\n  while True:\n    try:\n      yield cursor.next()\n    except tweepy.RateLimitError:\n      time.sleep(15 * 60)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["friend_count=0\nuser_name=\"hugobowne\"\nfor friend in limit_handled(tweepy.Cursor(api.friends, \n                                          screen_name=user_name, \n                                          rpp=100\n                                         ).items()):  \n  process_or_store_following(friend._json)\n  friend_count+=1\n  print(\"Downloaded {0} friends\".format(friend_count))"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["Save `following` as `pickle` file to get easy access to the data."],"metadata":{}},{"cell_type":"code","source":["import pickle\n\npickle.dump(following, open( \"/dbfs/FileStore/tmp/following.pkl\", \"wb\" ) )"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["%sh ls -lh /dbfs/FileStore/tmp/*.pkl"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["Loading saved `pickle` file."],"metadata":{}},{"cell_type":"code","source":["import pickle\n\nhugo_following = pickle.load(open( \"/dbfs/FileStore/tmp/following.pkl\", \"rb\" ) )"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["len(hugo_following), type(hugo_following), type(hugo_following[0])"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["hugo_following[0]"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["We can use `pandas` library to create dataframe from list of dictionary by assigning column names to `columns` parameter."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\n\ncol_names=['name', 'screen_name','followers_count', 'friends_count', 'favourites_count', \n           'status_count','created_at','language','location','id']\nhugo_following_data = pd.DataFrame.from_records(hugo_following, columns=col_names)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["hugo_following_data.shape"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["And save the dataframe as pickle file"],"metadata":{}},{"cell_type":"code","source":["import pickle\n\npickle.dump(hugo_following_data, open( \"/dbfs/FileStore/tmp/hugo_following_data.pkl\", \"wb\" ) )"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["`.friends_ids()` function returns only user id of specific user, where user id and screen name can be used. This function perform faster than `.friends()` function because it only returns user id and it doesn't require cursors to navigate.\n\n`.get_user()` function returns information about specific user, where user id and screen name can be used."],"metadata":{}},{"cell_type":"markdown","source":["![alt](https://i.imgur.com/2rQYdpU.jpg)"],"metadata":{}},{"cell_type":"markdown","source":["Here we will extract user id of people who are followed by users that Hugo follows as illustrate in the schema above. Following function creates list of tuples with two elements. The first element is users followed by Hugo, and the second is dictionary of users followed by the first element. We use `.friends_ids()` function to get user id of the second users, then use `.get_user()` function to extract user information of the second users."],"metadata":{}},{"cell_type":"code","source":["write_edgelist=[]\ndef second_friend(json):\n  count=0\n  for friend in json:\n    friend_screen_name=friend['screen_name']\n    id_list = api.friends_ids(screen_name=friend_screen_name)\n    count+=1\n    count_fol=0\n    for second_id in id_list:\n      second_user=api.get_user(id=second_id)\n      second_screen_name=second_user.screen_name\n      write_edgelist.append((friend_screen_name, second_user))\n      count_fol+=1\n      print(\"Friend {}\".format(count), \"following {} downloaded\".format(count_fol))"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["second_friend(hugo_following)"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["Since the second element is dictionary, we can extract any information related to the second element. This comes in handy when we do network analysis where we can analyze followee/follower relationship. In our case, the first element is the follower, the second element is followee and the extra information can be stored as node's metadata."],"metadata":{}},{"cell_type":"code","source":["write_edgelist[0]"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["We can save this as pickle file."],"metadata":{}},{"cell_type":"code","source":["import pickle\n\npickle.dump(write_edgelist, open( \"/dbfs/FileStore/tmp/hugo_write_edgelist.pkl\", \"wb\" ) )"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":["###3. Geo methods"],"metadata":{}},{"cell_type":"markdown","source":["`.geo_search()` helps us to extract location id."],"metadata":{}},{"cell_type":"code","source":["places = api.geo_search(query=\"USA\", granularity=\"country\")\nplace_id = places[0].id\nprint('USA id is: ',place_id)"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":["The USA's location ID is `96683cc9126741d1` which will allow us to extract tweets originated in the USA."],"metadata":{}},{"cell_type":"markdown","source":["The following function extracts tweets and navigate to the next cursor."],"metadata":{}},{"cell_type":"code","source":["import json\nimport time\n\ngeo_usa=[]\ndef process_or_store_geo(place):\n    geo_usa.append(place)\n    \ndef limit_handled(cursor):\n  while True:\n    try:\n      yield cursor.next()\n    except tweepy.RateLimitError:\n      time.sleep(15 * 60)"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":["`.search()` function returns tweets that match a specified query. We can use place ID to collect tweets originated in the USA by including `'place:96683cc9126741d1'` to the search query. We can set up maximum number of tweets to extract."],"metadata":{}},{"cell_type":"code","source":["searchquery='place:96683cc9126741d1'\nmax_tweets=10000\ntweet_count=0\n\nfor tweet in limit_handled(tweepy.Cursor(api.search,q=searchquery).items(max_tweets)) :         \n  if tweet.place is not None:\n    process_or_store_geo(tweet._json)\n    tweet_count += 1\n    print(\"Downloaded {0} tweets\".format(tweet_count))"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"markdown","source":["Saving geo_usa as pickle file."],"metadata":{}},{"cell_type":"code","source":["import pickle\n\npickle.dump(geo_usa, open( \"/dbfs/FileStore/tmp/geo_usa.pkl\", \"wb\" ))"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["%sh ls -lh /dbfs/FileStore/tmp/*.pkl"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"markdown","source":["Loading `geo_usa` pickle file as `geo_usa_location`."],"metadata":{}},{"cell_type":"code","source":["import pickle\n\ngeo_usa_location = pickle.load(open( \"/dbfs/FileStore/tmp/geo_usa.pkl\", \"rb\" ))"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["len(geo_usa_location)"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"markdown","source":["We can use the coodinates to map the origins of the tweets."],"metadata":{}},{"cell_type":"code","source":["geo_usa[0]['place']['bounding_box']['coordinates']"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"markdown","source":["##2.3. Extracting old tweets"],"metadata":{}},{"cell_type":"markdown","source":["We can use `.search()` to extract old tweets."],"metadata":{}},{"cell_type":"code","source":["import json\nimport time\n\nold_tweets=[]\ndef process_or_store(tweet):\n    old_tweets.append(json.dumps(tweet))"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"markdown","source":["For example, here we extract tweets with `google` word in the text and written in English. We can also specify whether to include entities which are hashtags, user mentions, links, stock tickers (symbols), Twitter polls, and attached media. We can set up maximum number of tweets to extract, otherwise this will never stop."],"metadata":{}},{"cell_type":"code","source":["tweet_count=0\nmax_count=10000\nfor tweet in tweepy.Cursor(api.search, \n                           q=\"google\", \n                           result_type=\"recent\", \n                           include_entities=True, \n                           lang=\"en\"\n                          ).items(max_count):\n    tweet_count+=1\n    process_or_store(tweet._json)\n    print(\"Downloaded {0} tweets\".format(tweet_count))"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"markdown","source":["We can save it as pickle file."],"metadata":{}},{"cell_type":"code","source":["import pickle\n\npickle.dump(old_tweets, open( \"/dbfs/FileStore/tmp/google_tweets.pkl\", \"wb\" ) )"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"code","source":["%sh ls -lh /dbfs/FileStore/tmp/*.pkl"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"code","source":["import pickle\ngoogle_tweets = pickle.load(open( \"/dbfs/FileStore/tmp/google_tweets.pkl\", \"rb\" ) )"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["google_tweets[0]"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"markdown","source":["The End"],"metadata":{}}],"metadata":{"name":"1. Tweepy Introduction","notebookId":105509},"nbformat":4,"nbformat_minor":0}