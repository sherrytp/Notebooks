{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Steam Games \n",
    "\n",
    "* This steam app project aims at understanding more about game types, game ratings, and the similarity. \n",
    "* The prediction on the game ratings has a relatively acceptable accuracy, reaching training error of 0.045461222106009884 and testing error of 0.10450519514048963. \n",
    "* Data can be found in https://cseweb.ucsd.edu/~jmcauley/datasets.html#steam_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import ast\n",
    "import collections\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_aus(filename):\n",
    "    users_db = {}\n",
    "    time_start = time.time()\n",
    "    sid_obj = SentimentIntensityAnalyzer() \n",
    "    \n",
    "    for line in open(filename, 'r', encoding='utf-8'):\n",
    "        user = ast.literal_eval(line)\n",
    "        \n",
    "        user_id = user['user_id'] \n",
    "        for i in range(len(user['reviews'])):\n",
    "            \n",
    "            product_id = user['reviews'][i]['item_id']\n",
    "            text = user['reviews'][i]['review']\n",
    "            rating = sid_obj.polarity_scores(text)['compound']\n",
    "            recommend = user['reviews'][i]['recommend']\n",
    "        \n",
    "            if user_id not in users_db:\n",
    "                users_db[user_id] = {}\n",
    "            users_db[user_id][product_id] = {}\n",
    "            users_db[user_id][product_id]['rating'] = rating\n",
    "            users_db[user_id][product_id]['recommend'] = recommend\n",
    "    \n",
    "    time_end = time.time()\n",
    "    print('Reading data: ' + str(time_end - time_start) + ' sec')\n",
    "    return users_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items_db(filename):\n",
    "    items_db = {}\n",
    "    time_start = time.time()\n",
    "    \n",
    "    for line in open(filename, 'r', encoding='utf-8'):\n",
    "        item = ast.literal_eval(line)\n",
    "        \n",
    "        if 'id' not in item or 'tags' not in item:\n",
    "            continue\n",
    "            \n",
    "        item_id = item['id'] \n",
    "        genre_list = item['tags']\n",
    "        \n",
    "        items_db[item_id] = genre_list\n",
    "        \n",
    "    time_end = time.time()\n",
    "    print('Reading data: ' + str(time_end - time_start) + ' sec')\n",
    "    return items_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data: 63.15352416038513 sec\n"
     ]
    }
   ],
   "source": [
    "users_db = get_users_aus(\"datasets/australian_user_reviews.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data: 2.4937798976898193 sec\n"
     ]
    }
   ],
   "source": [
    "items_db = get_items_db(\"datasets/steam_games.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER accuracy (ignore 0): 0.8077235349224524\n",
      "VADER cannot decide rate: 0.17791925518988208\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "false_count = 0\n",
    "cannot_decide_count = 0\n",
    "\n",
    "for user in users_db:\n",
    "    for item in users_db[user]:\n",
    "        rating = users_db[user][item]['rating']\n",
    "        recommend = users_db[user][item]['recommend']\n",
    "        if rating == 0:\n",
    "            cannot_decide_count += 1\n",
    "        elif (rating > 0 and recommend == True) or (rating < 0 and recommend == False):\n",
    "            correct_count += 1\n",
    "        else:\n",
    "            false_count += 1\n",
    "            \n",
    "print('VADER accuracy (ignore 0):', correct_count / (correct_count + false_count))\n",
    "print('VADER cannot decide rate:', cannot_decide_count/ (correct_count + false_count + cannot_decide_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_game = set()\n",
    "for user in users_db:\n",
    "    for item in users_db[user]:\n",
    "        all_game.add(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exist_num = 0\n",
    "not_exist_num = 0\n",
    "for game in all_game:\n",
    "    if game in items_db:\n",
    "        exist_num += 1\n",
    "    else:\n",
    "        not_exist_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game has type rate: 0.8671917436175991\n"
     ]
    }
   ],
   "source": [
    "print('Game has type rate:', exist_num / (exist_num + not_exist_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([13529.,  4629.,  2529.,  1499.,   980.,   736.,   551.,   419.,\n",
       "          324.,   262.]),\n",
       " array([ 1. ,  1.9,  2.8,  3.7,  4.6,  5.5,  6.4,  7.3,  8.2,  9.1, 10. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = []\n",
    "for i in users_db:\n",
    "    count.append(len(users_db[i]))\n",
    "plt.hist(count,edgecolor='k',alpha=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_to_rating_dict(users_db_name, user_based=True):\n",
    "    item_set = set()\n",
    "    user_to_rating = collections.defaultdict(dict)\n",
    "    \n",
    "    for user in users_db:\n",
    "        for item in users_db[user]:\n",
    "            if user_based:\n",
    "                userId = user\n",
    "                itemId = item\n",
    "            else: # Hack for item-based method: Swap definition of user/item.\n",
    "                userId = item\n",
    "                itemId = user\n",
    "            rating = users_db[user][item]['rating']\n",
    "\n",
    "            user_to_rating[user][item] = rating\n",
    "            item_set.add(item)\n",
    "\n",
    "    return user_to_rating, item_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_rating, item_set = get_user_to_rating_dict(users_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split data into train and test, remove cannot count review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_clean = [((review,r), k) for k, v in user_to_rating.items() for review,r in v.items() if r!=0.0] #remove vader=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(list(user_clean), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set(train):\n",
    "    train_set=collections.defaultdict(dict)\n",
    "    for i in train:       \n",
    "        train_set[i[1]][i[0][0]] = i[0][1]\n",
    "    return train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user = get_set(user_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = get_set(train)\n",
    "test_db = get_set(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43231"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train) # make sure number of review is same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43231"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count1 = []\n",
    "for i in train_db:\n",
    "    count1.append(len(train_db[i]))\n",
    "sum(count1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4804"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4804"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count2 = []\n",
    "for i in test_db:\n",
    "    count2.append(len(test_db[i]))\n",
    "sum(count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = [i for i in train_db]\n",
    "test_id = [i for i in test_db]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_item = [list(v.keys()) for user,v in train_db.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item = [list(v.keys()) for user,v in test_db.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User_Base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_to_rating_dict(users_db_name, user_based=True):\n",
    "    item_set = set()\n",
    "    user_to_rating = collections.defaultdict(dict)\n",
    "    \n",
    "    for user in users_db:\n",
    "        for item in users_db[user]:\n",
    "            if user_based:\n",
    "                userId = user\n",
    "                itemId = item\n",
    "            else: # Hack for item-based method: Swap definition of user/item.\n",
    "                userId = item\n",
    "                itemId = user\n",
    "            rating = users_db[user][item]['rating']\n",
    "\n",
    "            user_to_rating[user][item] = rating\n",
    "            item_set.add(item)\n",
    "\n",
    "    return user_to_rating, item_set\n",
    "\n",
    "def get_similarity_dict(db, id_list, similarity_func):\n",
    "    # sim_db map user_id to its K_NEIGHBORS, sorted by similarity\n",
    "    sim_db = {}\n",
    "    n = 0\n",
    "    # You could use tqdm to make a progressbar: https://github.com/tqdm/tqdm\n",
    "    for target_user in id_list:\n",
    "        sim_ary = []\n",
    "        target_dict = db[target_user]\n",
    "        for current_user in db:\n",
    "            if current_user != target_user:\n",
    "                current_dict = db[current_user]\n",
    "                sim = similarity_func(target_dict, current_dict)\n",
    "                if sim > 0:\n",
    "                    sim_ary.append([current_user, sim])\n",
    "\n",
    "        sim_ary.sort(key=lambda x: x[1], reverse=True)\n",
    "        sim_db[target_user] = sim_ary\n",
    "        n += 1\n",
    "        if n % 500 == 0:\n",
    "            print('finish', n)\n",
    "    \n",
    "    return sim_db\n",
    "\n",
    "def cosine_similarity(d1, d2):\n",
    "    common_key = d1.keys() & d2.keys() #intersection\n",
    "    if len(common_key) > 0:\n",
    "        dot = sum([d1[k] * d2[k] for k in common_key])\n",
    "        norm1 = np.linalg.norm(list(d1.values())) #by default, L2 norm\n",
    "        norm2 = np.linalg.norm(list(d2.values()))\n",
    "        norm_12 = norm1 * norm2\n",
    "        if norm_12 == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            sim = dot / norm_12\n",
    "            return sim\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def centred_cosine_similarity(d1, d2):\n",
    "    m1 = np.mean(list(d1.values()))\n",
    "    m2 = np.mean(list(d2.values()))\n",
    "\n",
    "    d1 = {k: v - m1 for k, v in d1.items()}\n",
    "    d2 = {k: v - m2 for k, v in d2.items()}\n",
    "\n",
    "    return cosine_similarity(d1, d2)\n",
    "\n",
    "def pearson_similarity(d1, d2):\n",
    "    common_key = d1.keys() & d2.keys()\n",
    "    if len(common_key) >= 2:\n",
    "        d1 = {k: v for k, v in d1.items() if k in common_key}\n",
    "        d2 = {k: v for k, v in d2.items() if k in common_key}\n",
    "        return centred_cosine_similarity(d1, d2)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def predict_ratings(db, sim_db, movie_list, test_user_ids, N_TOP, K_NEIGHBORS):\n",
    "    rec_db = {}\n",
    "    \n",
    "    for test_user_id in test_user_ids: # For each user in the test set **use train user**\n",
    "        rec_ary = [] # List of recommended movies\n",
    "        r_a = np.mean(list(db[test_user_id].values())) # See slides 21. Average rating for user a.\n",
    "        #if db[test_user_id]=={}:\n",
    "            #rec_ary.append([movie_id,average_score])\n",
    "        #else:\n",
    "        for movie_id in movie_list: # Predict rating for all unseen movies\n",
    "            #if movie_id not in db[test_user_id]: #only predicts movies user don't have a rating \n",
    "            if movie_id in db[test_user_id]: # for train, we only predict the movies user had a rating\n",
    "                num_ary = [] #each term in numerator\n",
    "                den_ary = [] #each term in denominator\n",
    "               \n",
    "                for user_id, sim in sim_db[test_user_id]: # Compare all neighbors, until K_NEIGHBORS\n",
    "                    if sim > 0: #Only consider positve similarity\n",
    "             \n",
    "                        r_ui = db[user_id].get(movie_id, None) #rating for user u for movie i\n",
    "                        if r_ui is not None: #Has rating\n",
    "                            r_u = np.mean([v for k, v in db[user_id].items() if k != movie_id]) #Average rating for user u. Exclude the movie being computed.\n",
    "\n",
    "                            num_ary.append((r_ui - r_u) * sim)\n",
    "                            den_ary.append(sim)\n",
    "                            if len(den_ary) >= K_NEIGHBORS: #Stop once reach K_NEIGHBORS\n",
    "                                    break\n",
    "\n",
    "                if len(den_ary) == 0: #No other user have viewed this movie\n",
    "                    p_ai = r_a\n",
    "                else:\n",
    "                    p_ai = r_a + sum(num_ary) / sum(den_ary)\n",
    "                \n",
    "                if p_ai > 1:\n",
    "                    p_ai = 1\n",
    "                elif p_ai < -1:\n",
    "                    p_ai = -1\n",
    "                #p_ai could be greater than 5! Because r_a could be already quite large (this user usually give high ratings)\n",
    "                #You should clip the rating to [0.0, 5.0] via np.clip(p_ai, 0, 5)\n",
    "                rec_ary.append([movie_id, p_ai])\n",
    "            \n",
    "        rec_ary.sort(key=lambda x: x[0], reverse=True) #for train, useing moveid to sort\n",
    "        \n",
    "        rec_db[test_user_id] = rec_ary #[:N_TOP] for prediction\n",
    "\n",
    "    return rec_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_rate(db, sim_db, K_NEIGHBORS, user_id, item_id):\n",
    "    r_a = np.mean(list(db[user_id].values()))\n",
    "    num_ary = [] #each term in numerator\n",
    "    den_ary = [] #each term in denominator\n",
    "    if user_id not in sim_db:\n",
    "        return 0.33\n",
    "    for neighbor_user_id, sim in sim_db[user_id]: # Compare all neighbors, until K_NEIGHBORS\n",
    "        r_ui = db[neighbor_user_id].get(item_id, None) #rating for user u for movie i\n",
    "        if r_ui is not None: #Has rating\n",
    "            r_u = np.mean([v for k, v in db[neighbor_user_id].items() if k != item_id]) #Average rating for user u. Exclude the movie being computed.\n",
    "            num_ary.append((r_ui - r_u) * sim)\n",
    "            den_ary.append(sim)\n",
    "            if len(den_ary) >= K_NEIGHBORS: #Stop once reach K_NEIGHBORS\n",
    "                break\n",
    "\n",
    "    if len(den_ary) == 0: #No other user have viewed this movie\n",
    "        p_ai = r_a\n",
    "    else:\n",
    "        p_ai = r_a + sum(num_ary) / sum(den_ary)\n",
    "\n",
    "    if p_ai > 1:\n",
    "        p_ai = 1\n",
    "    elif p_ai < -1:\n",
    "        p_ai = -1\n",
    "\n",
    "    return p_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 500\n",
      "finish 1000\n",
      "finish 1500\n",
      "finish 2000\n",
      "finish 2500\n",
      "finish 3000\n",
      "finish 3500\n",
      "finish 4000\n",
      "finish 4500\n",
      "finish 5000\n",
      "finish 5500\n",
      "finish 6000\n",
      "finish 6500\n",
      "finish 7000\n",
      "finish 7500\n",
      "finish 8000\n",
      "finish 8500\n",
      "finish 9000\n",
      "finish 9500\n",
      "finish 10000\n",
      "finish 10500\n",
      "finish 11000\n",
      "finish 11500\n",
      "finish 12000\n",
      "finish 12500\n",
      "finish 13000\n",
      "finish 13500\n",
      "finish 14000\n",
      "finish 14500\n",
      "finish 15000\n",
      "finish 15500\n",
      "finish 16000\n",
      "finish 16500\n",
      "finish 17000\n",
      "finish 17500\n",
      "finish 18000\n",
      "finish 18500\n",
      "finish 19000\n",
      "finish 19500\n",
      "finish 20000\n",
      "finish 20500\n",
      "finish 21000\n",
      "Reading data: 408.18937706947327 sec\n"
     ]
    }
   ],
   "source": [
    "# tune paremeters N-top, K and sim_function\n",
    "time_start = time.time()\n",
    "SIM_FUNCTION = pearson_similarity\n",
    "\n",
    "item_list = list(item_set)\n",
    "item_list.sort()\n",
    "\n",
    "sim_db = get_similarity_dict(train_db, train_id, SIM_FUNCTION)\n",
    "\n",
    "time_end = time.time()\n",
    "print('Reading data: ' + str(time_end - time_start) + ' sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_NEIGHBORS = 50\n",
    "SE = 0\n",
    "n = 0\n",
    "for user in train_db:\n",
    "    for item in train_db[user]:\n",
    "        pred_value = predict_single_rate(train_db, sim_db, K_NEIGHBORS, user, item)\n",
    "        real_value = train_db[user][item]\n",
    "        SE += (pred_value - real_value) ** 2\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12635184688184897"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SE / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE = 0\n",
    "n = 0\n",
    "for user in test_db:\n",
    "    for item in test_db[user]:\n",
    "        pred_value = predict_single_rate(train_db, sim_db, K_NEIGHBORS, user, item)\n",
    "        real_value = test_db[user][item]\n",
    "        SE += (pred_value - real_value) ** 2\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3611510920070702"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SE / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_id = [i for i in all_user]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings_2(db, sim_db, movie_list, test_user_ids, N_TOP, K_NEIGHBORS):\n",
    "    rec_db = {}\n",
    "    \n",
    "    for test_user_id in test_user_ids: # For each user in the test set **use train user**\n",
    "        rec_ary = [] # List of recommended movies\n",
    "        r_a = np.mean(list(db[test_user_id].values())) # See slides 21. Average rating for user a.\n",
    "        #if db[test_user_id]=={}:\n",
    "            #rec_ary.append([movie_id,average_score])\n",
    "        #else:\n",
    "        for movie_id in movie_list: # Predict rating for all unseen movies\n",
    "            if movie_id not in db[test_user_id]: #only predicts movies user don't have a rating \n",
    "            #if movie_id in db[test_user_id]: # for train, we only predict the movies user had a rating\n",
    "                num_ary = [] #each term in numerator\n",
    "                den_ary = [] #each term in denominator\n",
    "               \n",
    "                for user_id, sim in sim_db[test_user_id]: # Compare all neighbors, until K_NEIGHBORS\n",
    "                    if sim > 0: #Only consider positve similarity\n",
    "             \n",
    "                        r_ui = db[user_id].get(movie_id, None) #rating for user u for movie i\n",
    "                        if r_ui is not None: #Has rating\n",
    "                            r_u = np.mean([v for k, v in db[user_id].items() if k != movie_id]) #Average rating for user u. Exclude the movie being computed.\n",
    "\n",
    "                            num_ary.append((r_ui - r_u) * sim)\n",
    "                            den_ary.append(sim)\n",
    "                            if len(den_ary) >= K_NEIGHBORS: #Stop once reach K_NEIGHBORS\n",
    "                                    break\n",
    "\n",
    "                if len(den_ary) == 0: #No other user have viewed this movie\n",
    "                    p_ai = r_a\n",
    "                else:\n",
    "                    p_ai = r_a + sum(num_ary) / sum(den_ary)\n",
    "                \n",
    "                if p_ai > 1:\n",
    "                    p_ai = 1\n",
    "                elif p_ai < -1:\n",
    "                    p_ai = -1\n",
    "                #p_ai could be greater than 5! Because r_a could be already quite large (this user usually give high ratings)\n",
    "                #You should clip the rating to [0.0, 5.0] via np.clip(p_ai, 0, 5)\n",
    "                rec_ary.append([movie_id, p_ai])\n",
    "            \n",
    "        rec_ary.sort(key=lambda x: x[0], reverse=True) #for train, useing moveid to sort\n",
    "        \n",
    "        rec_db[test_user_id] = rec_ary[:N_TOP] \n",
    "\n",
    "    return rec_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 500\n",
      "finish 1000\n",
      "finish 1500\n",
      "finish 2000\n",
      "finish 2500\n",
      "finish 3000\n",
      "finish 3500\n",
      "finish 4000\n",
      "finish 4500\n",
      "finish 5000\n",
      "finish 5500\n",
      "finish 6000\n",
      "finish 6500\n",
      "finish 7000\n",
      "finish 7500\n",
      "finish 8000\n",
      "finish 8500\n",
      "finish 9000\n",
      "finish 9500\n",
      "finish 10000\n",
      "finish 10500\n",
      "finish 11000\n",
      "finish 11500\n",
      "finish 12000\n",
      "finish 12500\n",
      "finish 13000\n",
      "finish 13500\n",
      "finish 14000\n",
      "finish 14500\n",
      "finish 15000\n",
      "finish 15500\n",
      "finish 16000\n",
      "finish 16500\n",
      "finish 17000\n",
      "finish 17500\n",
      "finish 18000\n",
      "finish 18500\n",
      "finish 19000\n",
      "finish 19500\n",
      "finish 20000\n",
      "finish 20500\n",
      "finish 21000\n",
      "finish 21500\n",
      "finish 22000\n"
     ]
    }
   ],
   "source": [
    "sim_all = get_similarity_dict(all_user, all_id, SIM_FUNCTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOP = 5\n",
    "prediction = predict_ratings_2(all_user, sim_all, item_list, all_id, N_TOP, K_NEIGHBORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result2 = pd.DataFrame(data=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = result2.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2.to_csv(\"user_base_result.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Content Base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_itemId_to_genres(items_db):\n",
    "    itemId_to_genres = {}\n",
    "\n",
    "    for itemId in items_db:\n",
    "        itemId_to_genres[itemId] = ''\n",
    "        for genre in items_db[itemId]:\n",
    "            if len(itemId_to_genres[itemId]) == 0:\n",
    "                itemId_to_genres[itemId] += genre\n",
    "            else:\n",
    "                itemId_to_genres[itemId] += ('|' + genre)\n",
    "\n",
    "    return itemId_to_genres\n",
    "\n",
    "def get_items_meta_db(movieId_to_genres):\n",
    "    index_to_movieId = {}\n",
    "    movieId_to_index = {}\n",
    "    genres_to_index = {}\n",
    "    movies_genres_set_list = []\n",
    "    index = 0\n",
    "\n",
    "    for k in movieId_to_genres.keys():\n",
    "        index_to_movieId[index] = k\n",
    "        movieId_to_index[k] = index\n",
    "\n",
    "        index += 1\n",
    "        genres = movieId_to_genres[k].split('|')\n",
    "        genres_set = set()\n",
    "        for genre in genres:\n",
    "            if genre not in genres_to_index.keys():\n",
    "                genres_to_index[genre] = len(genres_to_index)\n",
    "            genres_set.add(genres_to_index[genre])\n",
    "        movies_genres_set_list.append(genres_set)\n",
    "\n",
    "    return index_to_movieId, movieId_to_index, genres_to_index, movies_genres_set_list\n",
    "\n",
    "def get_ratings_meta_db(users_db):\n",
    "    userId_to_itemId = {}\n",
    "    userId_itemId_to_rating = {}\n",
    "    userId_to_sum_count = {}\n",
    "    \n",
    "    for user in users_db:\n",
    "        userId_to_itemId[user] = []\n",
    "        userId_to_sum_count[user] = [0, 0]\n",
    "        for item in users_db[user]:\n",
    "            rating = users_db[user][item]['rating']\n",
    "            \n",
    "            userId_to_itemId[user].append(item)\n",
    "            userId_itemId_to_rating[(user, item)] = rating\n",
    "            userId_to_sum_count[user][0] += rating\n",
    "            userId_to_sum_count[user][1] += 1\n",
    "            \n",
    "    return userId_to_itemId, userId_itemId_to_rating, userId_to_sum_count\n",
    "\n",
    "def get_hash_coeffs(br):\n",
    "    rnds = np.random.choice(2**8, (2, br), replace=False)\n",
    "    c = 347 \n",
    "    return rnds[0], rnds[1], c\n",
    "\n",
    "def min_hashing(shingles, hash_coeffs, br):\n",
    "    count = len(shingles)\n",
    "    (a, b, c) = hash_coeffs\n",
    "    a = a.reshape(1, -1)\n",
    "    M = np.zeros((br, count), dtype=int) #Its layout same as slide 56. col are docs, row are signature index\n",
    "    for i, s in enumerate(shingles):\n",
    "        # All shingles in the document\n",
    "        row_idx = np.asarray(list(s)).reshape(-1, 1)\n",
    "        # Instead of getting many hash functions and run each hash function to each shingles,\n",
    "        # Use numpy matrix multiplication to apply all hash funcitons to all shingles in the same time\n",
    "        m = (np.matmul(row_idx, a) + b) % c\n",
    "        m_min = np.min(m, axis=0) #For each hash function, minimum hash value for all shingles\n",
    "        M[:, i] = m_min\n",
    "\n",
    "    return M\n",
    "\n",
    "def LSH(M, b, r, band_hash_size):\n",
    "    count = M.shape[1]\n",
    "\n",
    "    bucket_list = []\n",
    "    for band_index in range(b):\n",
    "        # The hash table for each band is stored as a dictionrary of sets. It's more efficient than sparse matrix\n",
    "        m = collections.defaultdict(set)\n",
    "\n",
    "        row_start = band_index * r\n",
    "        for c in range(count):\n",
    "            v = M[row_start:(row_start+r), c]\n",
    "            v_hash = hash(tuple(v.tolist())) % band_hash_size\n",
    "            m[v_hash].add(c)\n",
    "\n",
    "        bucket_list.append(m)\n",
    "\n",
    "    return bucket_list\n",
    "\n",
    "def get_similar_moiveIndex_set(movieIndex):\n",
    "    similar_set = set()\n",
    "    for i in range(0, len(bucket_list)):\n",
    "        cur_bucket = bucket_list[i]\n",
    "        bucketIndex = itemIndex_to_bucketIndex[i][movieIndex]\n",
    "        for num in cur_bucket[bucketIndex]:\n",
    "            similar_set.add(num)\n",
    "    return similar_set\n",
    "\n",
    "def content_based_pred_rate(userId, movieId, userId_to_moiveId, userId_to_sum_count, userId_moiveId_to_rating, movieId_to_index, M):\n",
    "    summation = 0\n",
    "    sim_sum = 0\n",
    "    if movieId in movieId_to_index:\n",
    "        movieIndex_predict = movieId_to_index[movieId]\n",
    "        similar_set = get_similar_moiveIndex_set(movieIndex_predict)\n",
    "    \n",
    "        for seen_id in userId_to_moiveId[userId]:\n",
    "            if seen_id in movieId_to_index:\n",
    "                seen_index = movieId_to_index[seen_id]\n",
    "                if seen_index not in similar_set:\n",
    "                    continue\n",
    "                seen_rate = userId_moiveId_to_rating[(userId, seen_id)]\n",
    "            \n",
    "                movie_index = movieId_to_index[movieId]\n",
    "                c1 = M[:, seen_index]\n",
    "                c2 = M[:, movie_index]\n",
    "                sim = np.mean(c1 == c2)\n",
    "\n",
    "                summation += sim * seen_rate\n",
    "                sim_sum += sim\n",
    "                \n",
    "    \n",
    "            if sim_sum == 0:\n",
    "                return userId_to_sum_count[userId][0] / userId_to_sum_count[userId][1]\n",
    "    \n",
    "            pred_rating = summation / sim_sum   \n",
    "    \n",
    "            if pred_rating > 1:\n",
    "                return 1\n",
    "            elif pred_rating < -1:\n",
    "                return -1\n",
    "            else:\n",
    "                return pred_rating \n",
    "    else:\n",
    "        return userId_to_sum_count[userId][0] / userId_to_sum_count[userId][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemId_to_genres = get_itemId_to_genres(items_db)\n",
    "index_to_itemId, itemId_to_index, genres_to_index, genres_set_list = get_items_meta_db(itemId_to_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_hash_size = 2**13\n",
    "b = 15\n",
    "r = 5\n",
    "br = b * r\n",
    "\n",
    "hash_coeffs = get_hash_coeffs(br)\n",
    "M = min_hashing(genres_set_list, hash_coeffs, br)\n",
    "bucket_list = LSH(M, b, r, band_hash_size)\n",
    "\n",
    "itemIndex_to_bucketIndex = []\n",
    "for i in range(len(bucket_list)):\n",
    "    itemIndex_to_bucketIndex.append({})\n",
    "    cur_bucket_list = bucket_list[i]\n",
    "    for key in cur_bucket_list:\n",
    "        cur_bucket = cur_bucket_list[key]\n",
    "        for item_index in cur_bucket:\n",
    "            itemIndex_to_bucketIndex[i][item_index] = key\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**split train set & test set for content base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_clean_db = [((review,r), k) for k, v in users_db.items() for review,r in v.items() if r['rating']!=0.0] #remove vader=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2, test2 = train_test_split(list(user_clean_db), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_2(train):\n",
    "    train_set = collections.defaultdict(dict)\n",
    "    for i in train:\n",
    "        train_set[i[1]][i[0][0]]={}\n",
    "        train_set[i[1]][i[0][0]]['rating']=i[0][1]['rating']\n",
    "        train_set[i[1]][i[0][0]]['recommend']=i[0][1]['recommend']\n",
    "    return train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set2 = get_set_2(train2)\n",
    "test_set2 = get_set_2(test2)\n",
    "train2_id = [i for i in train_set2]\n",
    "userId_to_itemId, userId_itemId_to_rating, userId_to_sum_count = get_ratings_meta_db(train_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_based_pred_rate('euzi', '767400', userId_to_itemId, userId_to_sum_count, userId_itemId_to_rating, itemId_to_index, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_rating2, item_set2 = get_user_to_rating_dict(train_set2,user_based=False)\n",
    "item_list2 = list(item_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_list = list(item_set)\n",
    "item_list.sort()\n",
    "N_TOP = 5\n",
    "ratings = {}\n",
    "for userId in train2_id:\n",
    "    rec_ary = []\n",
    "    for movieId in item_list2:\n",
    "        if movieId in train_set2[userId]: #for train, for prediction use not in\n",
    "            rating=content_based_pred_rate(userId, movieId, userId_to_itemId, userId_to_sum_count, userId_itemId_to_rating, itemId_to_index, M)\n",
    "            rec_ary.append([movieId, rating])\n",
    "\n",
    "    #rec_ary.sort(key=lambda x: x[1], reverse=True)\n",
    "    ratings[userId] = rec_ary #rec_ary[:N_TOP] for prediction\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ratings:\n",
    "    ratings[i] = sorted(dict(ratings[i]).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_set2:\n",
    "    train_set2[i] = sorted(train_set2[i].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_pred = []\n",
    "for i in ratings:\n",
    "    for r in ratings[i]:\n",
    "        r_pred.append(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_acutal = []\n",
    "for i in train_set2:\n",
    "    for r in train_set2[i]:\n",
    "        r_acutal.append(r[1]['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set error=0.045461222106009884\n"
     ]
    }
   ],
   "source": [
    "print('train set error={}'.format(mean_squared_error(r_pred, r_acutal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set2 = get_set_2(test2)\n",
    "test2_id = [i for i in test_set2]\n",
    "userId_to_itemId2, userId_itemId_to_rating2, userId_to_sum_count2 = get_ratings_meta_db(test_set2)\n",
    "#user_to_rating, item_set = get_user_to_rating_dict(test_set2,user_based=False)\n",
    "#item_list = list(item_set)\n",
    "#item_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOP = 5\n",
    "ratings2 = {}\n",
    "average_score = 0.333\n",
    "for userId in test2_id:\n",
    "    rec_ary2 = []\n",
    "    for movieId in item_list:\n",
    "        if movieId in test_set2[userId]:\n",
    "            if userId in userId_to_sum_count:\n",
    "                rating2=content_based_pred_rate(userId, movieId, userId_to_itemId2, userId_to_sum_count, userId_itemId_to_rating2, itemId_to_index, M)\n",
    "                rec_ary2.append([movieId, rating2])\n",
    "            else:\n",
    "                rec_ary2.append([movieId,average_score])\n",
    "    #rec_ary.sort(key=lambda x: x[1], reverse=True)\n",
    "    ratings2[userId] = rec_ary2 #rec_ary[:N_TOP] for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ratings2:\n",
    "    ratings2[i] = sorted(dict(ratings2[i]).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_set2:\n",
    "    test_set2[i] = sorted(test_set2[i].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_pred2 = []\n",
    "for i in ratings2:\n",
    "    for r in ratings2[i]:\n",
    "        r_pred2.append(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_acutal2 = []\n",
    "for i in test_set2:\n",
    "    for r in test_set2[i]:\n",
    "        r_acutal2.append(r[1]['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set error=0.10450519514048963\n"
     ]
    }
   ],
   "source": [
    "print('test set error={}'.format(mean_squared_error(r_pred2, r_acutal2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_set = get_set_2(user_clean_db)\n",
    "all_id = [i for i in all_set]\n",
    "userId_to_itemId, userId_itemId_to_rating, userId_to_sum_count = get_ratings_meta_db(all_set)\n",
    "N_TOP = 5\n",
    "item_list = list(item_set)\n",
    "prediction = {}\n",
    "average_score = 0.333\n",
    "for userId in all_id:\n",
    "    rec = []\n",
    "    for movieId in item_list:\n",
    "        if movieId not in all_set[userId]:\n",
    "            if userId in userId_to_sum_count:\n",
    "                rating2=content_based_pred_rate(userId, movieId, userId_to_itemId, userId_to_sum_count, userId_itemId_to_rating, itemId_to_index, M)\n",
    "                rec.append([movieId, rating2])\n",
    "            else:\n",
    "                rec.append([movieId,average_score])\n",
    "    #rec_ary.sort(key=lambda x: x[1], reverse=True)\n",
    "    prediction[userId] = rec[:N_TOP] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
